---
title: "CLASS EXERCISE 2"
author: "Lara Filippone-Cristina Pozzoli"
date: "2022-12-04"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Task 1

We can see that for Beppe Grillo's blog there isn't any robots.txt file. This means that we can proceed with our scraping, but we must do it "politely", identyfing ourselves with our email address and our R version.

## Task 2

We "politely" download the webpage "<http://www.beppegrillo.it/un-mare-di-plastica-ci-sommergera/>"

```{r}
library(RCurl)
url <- "https://beppegrillo.it/un-mare-di-plastica-ci-sommergera/"
resp <- getURL(url=url, httpheader= c(from ="cristina.pozzoli@icloud.com", 
                              'User-Agent' = R.Version()$version.string))
```

## Task 3

We create a data frame with all the HTML links in the page, using the XML: :getHTMLLinks function.
Then, we use a regular expression to keep only those links that redirect to other posts of Beppe Grillo's blog.

```{r}
library(XML)
links <- getHTMLLinks(doc=resp, externalOnly = T, relative = F)
links
links2<-as.data.frame(links)
link_unl <- unlist(links2)

library(stringr)

str_view_all(link_unl, "https://beppegrillo\\.it/.+")

links_blog <- str_extract_all(link_unl, "https://beppegrillo\\.it/.+")
links_blog_unl <- unlist(links_blog)
links_blog1 <- as.data.frame(links_blog_unl)


links_blog2 <- str_extract_all(link_unl, "https://beppegrillo\\.it/[^category].+")
links_blog_unl2 <- unlist(links_blog2)
links_blog2<- as.data.frame(links_blog_unl2)
```

## Task 4

We go to the link https://beppegrillo.it/category/archivio/2016/ , which contains the entire blog for 2016. First we get all the links to each of the 47 pages, then we scrape the main text for each blog post.

``` {r}
url2 <- "https://beppegrillo.it/category/archivio/2016/"

resp2 <- getURL(url=url2, httpheader= c(from ="cristina.pozzoli@icloud.com", 
                                      'User-Agent' = R.Version()))
resp2

library(rvest)
library(XML)
library(tidyverse)

webpages <- str_c("https://beppegrillo.it/category/archivio/2016/page/", 1:47) %>%
  paste0("/")
webpages

download_politely <- function(from_url, to_html, my_email, my_agent = R.Version()$version.string) {
  
  require(httr)
   stopifnot(is.character(from_url))
  stopifnot(is.character(to_html))
  stopifnot(is.character(my_email))
  simps_req <- httr::GET(url = from_url, 
                         add_headers(
                           From = my_email, 
                           `User-Agent` = R.Version()$version.string
                         )
  )
  if (httr::http_status(simps_req)$message == "Success: (200) OK") {
    bin <- content(simps_req, as = "raw")
    writeBin(object = bin, con = to_html)
  } else {
    cat("Houston, we have a problem!")
  }
}

download_politely(from_url = url2, 
                  to_html = here::here("beppegrillo_polite.html"), 
                  my_email = "cristina.pozzoli@icloud.com")
dir.create("folder_beppe")
for (i in seq_along(webpages)) {
  cat(i, " ")
  
  download_politely(from_url = webpages[i], 
                    to_html = here::here("folder_beppe", str_c("page_",i,".html")), 
                    my_email = "cristina.pozzoli@icloud.com")
  
  Sys.sleep(2)
}
```

# Task 5

In computer science, "crawling" means to automatically and systematically follow a huge amount of hyperlinks by browsing and downloading the webpages they refer to. Crawlers or web spiders are programs which are designed to execute this kind of task and are typically used by search engines for web indexing, through which they can provide in a very short time a well organized and appropriate list of webpages in response to our queries.

The scraper we built is different from a crawler because we manually collected and provided a specific website to our scraper and then browsed and downloaded some of its webpages to scrape target information we were interested in. On the other hand, crawlers are not so selective and they are able to work automatically: they start from a hyperlink and then proceed to follow every other hyperlink they encounter, even those referring to external webpages.

If we wanted to build a crawler with the RCrawler package, we would use its main function "rcrawler". By providing the website URL and the Xpath or CSS selector patterns, this function can crawl the whole website, download webpages, and scrape its contents in an automated manner to produce a structured dataset:

```{r}
library(Rcrawler)
Rcrawler(Website, no_cores, no_conn, MaxDepth, DIR, RequestsDelay = 0,
  Obeyrobots = FALSE, Useragent, use_proxy = NULL, Encod,
  Timeout = 5, URLlenlimit = 255, urlExtfilter, dataUrlfilter,
  crawlUrlfilter, crawlZoneCSSPat = NULL, crawlZoneXPath = NULL,
  ignoreUrlParams, ignoreAllUrlParams = FALSE, KeywordsFilter,
  KeywordsAccuracy, FUNPageFilter, ExtractXpathPat, ExtractCSSPat,
  PatternsNames, ExcludeXpathPat, ExcludeCSSPat, ExtractAsText = TRUE,
  ManyPerPattern = FALSE, saveOnDisk = TRUE, NetworkData = FALSE,
  NetwExtLinks = FALSE, statslinks = FALSE, Vbrowser = FALSE,
  LoggedSession)
```

The main arguments would be the root URL of the website to crawl and scrape (Website) and the Xpath or CSS patterns of page sections from where the crawler should gather links to be followed (crawlZoneXPath or crawlZoneCSSPat).